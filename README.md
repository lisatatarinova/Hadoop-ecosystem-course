### Список решенных в рамках курса задач.

#### MapReduce

###### task1.

Список идентификаторов перемешайте в случайном порядке. Далее в каждой строке запишите через запятую случайное число идентификаторов - от 1 до 5.
Вариант способа перемешивания записей: дописать к каждой случайное число, отсортировать по нему весь список, потом число отбросить.

*Входные данные: список идентификаторов.*
*Формат вывода: id1,id2,...*
*Вывод на печать: первые 50 строк.*

###### task2.

Найдите слова, которые встречались в наибольшем числе документов.
Очистите слова от знаков пунктуации. Отсортируйте слова по числу документов, при равном числе - лексикографически.

*Входные данные: википедия.*
*Формат вывода: слово  число документов.*
*Вывод на печать: топ10 слов*

#### Hive

###### task1.

Создайте внешние (`EXTERNAL`) таблицы по исходным данным. В результате будет 4 таблицы: логи пользователей, данные ip адресов, данные пользователей и подсети. Из таблицы логов перенесите данные в другую таблицу, партицированную по датам – одна партиция на каждый день. На партиционированных таблицах и нужно будет выполнять запросы в следующих задачах.
Требуется, чтобы сериализация и десериализация данных осуществлялась с использованием регулярных выражений (см. `org.apache.hadoop.hive.serde2.RegexSerDe`).
Проверить правильность создания таблиц с помощью простейших запросов (`SELECT * FROM <table> LIMIT 10`). Эти Select запросы нужно также добавлять в скрипт задачи.

###### task2.

Напишите запрос, выбирающий количество посещений для каждого дня. Полученные результаты отсортируйте по убыванию количества.

###### task3.

Напишите запрос, выбирающий количество посещений от мужчин и от женщин по регионам.

###### task4.

Представьте ситуацию, что все новостные сайты переехали в домен `.com`. Вас попросили обновить базу логов, чтоб логи пользователей указывали не на старые домены, а на новые. Например, новостная ссылка http://news.rambler.ru/8744806 теперь должна выглядеть в ваших запросах как http://news.rambler.com/8744806. Используйте стриминг в `hive-sql` запросе. (Рекомендуется обратить внимание на команды `awk` и `sed`). Выведите TOP-10 записей логов без сортировки.

#### Spark

###### task1.

Найдите все пары двух последовательных слов (биграмм), где первое слово «*narodnaya*». Для каждой пары подсчитайте количество вхождений в тексте статей Википедии. Выведите все пары с их частотой вхождений в лексикографическом порядке. Формат вывода - `word_pair  count`.